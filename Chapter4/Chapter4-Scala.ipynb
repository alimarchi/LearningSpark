{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ad9e30",
   "metadata": {},
   "source": [
    "### Spark SQL and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a142dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://L2203100.bosonit.local:4041\n",
       "SparkContext available as 'sc' (version = 3.0.3, master = local[*], app id = local-1651735049543)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3855a2cf\r\n",
       "csvFile: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\r\n",
       "df: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession \n",
    "\n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    " .getOrCreate()\n",
    "\n",
    "// Path to data set \n",
    "val csvFile=\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "// Read and create a temporary view\n",
    "val df = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csvFile)\n",
    "\n",
    "// Create a temporary view\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e844cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7019d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f506ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb43f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Column] = [name: string, description: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Viewing the metadata\n",
    "\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28faa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usFlightsDF: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n",
       "usFlightsDF2: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// You can simply use SQL to query the table and assign the returned result to a DataFrame\n",
    "\n",
    "val usFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "val usFlightsDF2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0362e3",
   "metadata": {},
   "source": [
    "DataFrameReader is the core construct for reading data from a data source into a DataFrame.\n",
    "\n",
    "DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa4f85",
   "metadata": {},
   "source": [
    "No schema is needed when reading from a static Parquet data source—the Parquet metadata usually contains the schema, so it’s inferred.\n",
    "\n",
    "Parquet is the default and preferred data source for Spark because it’s efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf8c707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://L2203100.bosonit.local:4041\n",
       "SparkContext available as 'sc' (version = 3.0.3, master = local[*], app id = local-1651577530857)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use Parquet\n",
    "\n",
    "val file = \"\"\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf10c5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.read.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b307ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use CSV\n",
    "\n",
    "val df3 = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"mode\", \"PERMISSIVE\")\n",
    " .load(\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaede288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use JSON\n",
    "\n",
    "val df4 = spark.read.format(\"json\")\n",
    " .load(\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c1c7e",
   "metadata": {},
   "source": [
    "DataFrameWriter does the reverse of its counterpart: it saves or writes data to a specified built-in data source. \n",
    "\n",
    "DataFrameWriter.format(args)\n",
    " .option(args)\n",
    " .bucketBy(args)\n",
    " .partitionBy(args)\n",
    " .save(path)\n",
    " \n",
    "DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61d01c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Parquet files into a DataFrame\n",
    "\n",
    "val file = \"\"\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051c4e7",
   "metadata": {},
   "source": [
    "There’s no need to supply the schema, because Parquet saves it as part of its metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02bb73cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading Parquet les into a Spark SQL table\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    " USING parquet\n",
    " OPTIONS (\n",
    " path \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\" )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52440434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a57cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Writing DataFrames to Parquet files\n",
    "\n",
    "df.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e75b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Writing DataFrames to Spark SQL tables\n",
    "\n",
    "df.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594da8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\n",
    "val df = spark.read.format(\"parquet\").load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5ce41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read a JSON file into a DataFrame\n",
    "\n",
    "val file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "val df = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4340b8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading a JSON file into a Spark SQL table\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    " USING json\n",
    " OPTIONS (\n",
    " path \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef25375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a57a4cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\r\n",
       "schema: String = DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading a CSV file into a DataFrame\n",
    "\n",
    "val file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "val schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "\n",
    "val df = spark.read.format(\"csv\")\n",
    " .schema(schema)\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"mode\", \"FAILFAST\") // Exit if any errors\n",
    " .option(\"nullValue\", \"\") // Replace any null data with quotes\n",
    " .load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79c8a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading a CSV file into a Spark SQL table\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    " USING csv\n",
    " OPTIONS (\n",
    " path \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n",
    " header \"true\",\n",
    " inferSchema \"true\",\n",
    " mode \"FAILFAST\"\n",
    " )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f20778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1653da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Writing DataFrames to CSV files\n",
    "\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "063fb0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading an ORC file into a DataFrame\n",
    "\n",
    "val file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "val df = spark.read.format(\"orc\").load(file)\n",
    "df.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad4a9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Writing DataFrames to ORC files\n",
    "\n",
    "df.write.format(\"orc\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/orc/df_orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a37f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "+------+-----+---------+----+-----+\n",
      "|height|width|nChannels|mode|label|\n",
      "+------+-----+---------+----+-----+\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |1    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "+------+-----+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.source.image\r\n",
       "imageDir: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\r\n",
       "imagesDF: org.apache.spark.sql.DataFrame = [image: struct<origin: string, height: int ... 4 more fields>, label: int]\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading an image file into a DataFrame\n",
    "\n",
    "import org.apache.spark.ml.source.image\n",
    "val imageDir = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "val imagesDF = spark.read.format(\"image\").load(imageDir)\n",
    "imagesDF.printSchema\n",
    "imagesDF.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    " \"label\").show(5, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6664a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|                path|    modificationTime|length|             content|label|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 55037|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54634|[FF D8 FF E0 00 1...|    1|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54624|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54505|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54475|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\r\n",
       "binaryFilesDF: org.apache.spark.sql.DataFrame = [path: string, modificationTime: timestamp ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reading a binary file into a DataFrame\n",
    "\n",
    "val path = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .load(path)\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5b21f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/tmp/output/fire_parquet\r\n",
       "df: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"\"\"C:/tmp/output/fire_parquet\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7a971a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW fire_tbl\n",
    " USING parquet\n",
    " OPTIONS (\n",
    " path \"C:/tmp/output/fire_parquet\" )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eea7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|      Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+---------+\n",
      "| 110660238|    60|      11021821|Medical Incident|03/07/2011|03/07/2011|                Fire|03/07/2011 04:23:...|0 Block of BORICA ST|  SF|  94127|      B09|         19|8442|               3|       2|            2|   true|         null|        1|   MEDIC|                         1|                     9|                 7|West of Twin Peaks|(37.7225922041408...| 110660238-60|      2.9|\n",
      "| 110660253|   B03|      11021831|          Alarms|03/07/2011|03/07/2011|                Fire|03/07/2011 05:13:...|1000 Block of HOW...|  SF|  94103|      B03|         01|2252|               3|       3|            3|  false|         null|        1|   CHIEF|                         3|                     3|                 6|   South of Market|(37.7795675906982...|110660253-B03|2.3833334|\n",
      "| 110660282|   E43|      11021856|Medical Incident|03/07/2011|03/07/2011|               Other|03/07/2011 06:55:...| PERSIA AV/MADRID ST|  SF|  94112|      B09|         43|6134|               3|       3|            3|   true|         null|        1|  ENGINE|                         1|                     9|                11|         Excelsior|(37.7216187707613...|110660282-E43|     1.65|\n",
      "| 110660322|    94|      11021888|Medical Incident|03/07/2011|03/07/2011|               Other|03/07/2011 08:49:...|1100 Block of HOW...|  SF|  94103|      B03|         01|2314|               3|       3|            3|   true|         null|        1|   MEDIC|                         3|                     2|                 6|   South of Market|(37.7768521020734...| 110660322-94|1.5666667|\n",
      "| 110670051|   B09|      11021963|          Alarms|03/08/2011|03/07/2011|                Fire|03/08/2011 07:36:...|5800 Block of MIS...|  SF|  94112|      B09|         33|6213|               3|       3|            3|  false|         null|        1|   CHIEF|                         3|                     9|                11|     Outer Mission|(37.709160030699,...|110670051-B09|      2.8|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+------------------+--------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM fire_tbl\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ca91cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+--------------------+--------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "|ALSUnit|             Address|       AvailableDtTm|Battalion| Box|  CallDate|CallFinalDisposition|CallNumber|        CallType|CallTypeGroup|City|    Delay|FinalPriority|FirePreventionDistrict|IncidentNumber|            Location|        Neighborhood|NumAlarms|OriginalPriority|Priority|        RowID|StationArea|SupervisorDistrict|UnitID|UnitSequenceInCallDispatch|UnitType| WatchDate|Zipcode|\n",
      "+-------+--------------------+--------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+--------------------+--------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "|  false|2000 Block of CAL...|01/11/2002 01:51:...|      B04|3362|01/11/2002|               Other|  20110016|  Structure Fire|         null|  SF|     2.95|            3|                     4|       2003235|(37.7895840679362...|     Pacific Heights|        1|               3|       3|020110016-T13|         38|                 5|   T13|                         2|   TRUCK|01/10/2002|  94109|\n",
      "|   true|0 Block of SILVER...|01/11/2002 03:01:...|      B10|6495|01/11/2002|               Other|  20110022|Medical Incident|         null|  SF|      4.7|            3|                    10|       2003241|(37.7337623673897...|Bayview Hunters P...|        1|               3|       3|020110022-M17|         42|                10|   M17|                         1|   MEDIC|01/10/2002|  94124|\n",
      "|   true|MARKET ST/MCALLIS...|01/11/2002 02:39:...|      B03|1455|01/11/2002|               Other|  20110023|Medical Incident|         null|  SF|2.4333334|            3|                     3|       2003242|(37.7811772186856...|          Tenderloin|        1|               3|       3|020110023-M41|         01|                 6|   M41|                         2|   MEDIC|01/10/2002|  94102|\n",
      "|  false|APPLETON AV/MISSI...|01/11/2002 04:16:...|      B06|5626|01/11/2002|               Other|  20110032|    Vehicle Fire|         null|  SF|      1.5|            3|                     6|       2003250|(37.7388432849018...|      Bernal Heights|        1|               3|       3|020110032-E11|         32|                 9|   E11|                         1|  ENGINE|01/10/2002|  94110|\n",
      "|  false|1400 Block of SUT...|01/11/2002 06:01:...|      B04|3223|01/11/2002|               Other|  20110043|          Alarms|         null|  SF|3.4833333|            3|                     4|       2003259|(37.7872890372638...|    Western Addition|        1|               3|       3|020110043-B04|         03|                 2|   B04|                         2|   CHIEF|01/10/2002|  94109|\n",
      "+-------+--------------------+--------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+--------------------+--------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = C:/tmp/output/fire_json\r\n",
       "df: org.apache.spark.sql.DataFrame = [ALSUnit: boolean, Address: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/tmp/output/fire_json\"\n",
    "val df = spark.read.format(\"json\").load(file)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc97cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

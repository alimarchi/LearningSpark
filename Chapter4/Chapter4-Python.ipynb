{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e5351a",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63a5af",
   "metadata": {},
   "source": [
    "Normally, in a standalone Spark application, you will create a SparkSession instance manually. However, in a Spark shell (or Databricks notebook), the SparkSession is created for you and accessible via the appropriately named variable spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5c5eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    " .getOrCreate())\n",
    "\n",
    "# Path to data set\n",
    "csv_file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csv_file))\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af182075",
   "metadata": {},
   "source": [
    "If you want to specify a schema, you can use a DDL-formatted string. For example:\n",
    "schema = \"'date' STRING, 'delay' INT, 'distance' INT, 'origin' STRING, 'destination' STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cf89549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a temporary view, we can issue SQL queries using Spark SQL.\n",
    "#These queries are no different from those you might issue against a SQL table.\n",
    "\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "FROM us_delay_flights_tbl \n",
    "WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c1ccd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All flights between San Francisco (SFO) and Chicago (ORD) with at least a 2-hour delay:\n",
    "\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER BY delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7cdb43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/19  09:25'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As an exercise, convert the date column into a readable format. \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define a UDF to convert the date format into a legible format.(UDF = user defined function)\n",
    "\n",
    "def to_date_format_udf(d_str):\n",
    "  l = [char for char in d_str]\n",
    "  return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])\n",
    "\n",
    "to_date_format_udf(\"02190925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04c4aa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.to_date_format_udf(d_str)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the UDF\n",
    "\n",
    "spark.udf.register(\"to_date_format_udf\", to_date_format_udf, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "154d7a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: int, distance: int, origin: string, destination: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read US departure flight data\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .schema(\"date STRING, delay INT, distance INT, origin STRING, destination STRING\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"path\", \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\")\n",
    "      .load())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d4fab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_format |\n",
      "+------------+\n",
      "|01/01  12:45|\n",
      "|01/02  06:00|\n",
      "|01/02  12:45|\n",
      "|01/02  06:05|\n",
      "|01/03  12:45|\n",
      "|01/03  06:05|\n",
      "|01/04  12:43|\n",
      "|01/04  06:05|\n",
      "|01/05  12:45|\n",
      "|01/05  06:05|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the UDF\n",
    "\n",
    "df.selectExpr(\"to_date_format_udf(date) as data_format\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b6908d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view to which we can issue SQL queries\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f6bda62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache Table so queries are expedient\n",
    "spark.sql(\"CACHE TABLE us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6c54109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|date    |delay|distance|origin|destination|date    |date_fm     |\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|01011245|6    |602     |ABE   |ATL        |01011245|01/01  12:45|\n",
      "|01020600|-8   |369     |ABE   |DTW        |01020600|01/02  06:00|\n",
      "|01021245|-2   |602     |ABE   |ATL        |01021245|01/02  12:45|\n",
      "|01020605|-4   |602     |ABE   |ATL        |01020605|01/02  06:05|\n",
      "|01031245|-4   |602     |ABE   |ATL        |01031245|01/03  12:45|\n",
      "|01030605|0    |602     |ABE   |ATL        |01030605|01/03  06:05|\n",
      "|01041243|10   |602     |ABE   |ATL        |01041243|01/04  12:43|\n",
      "|01040605|28   |602     |ABE   |ATL        |01040605|01/04  06:05|\n",
      "|01051245|88   |602     |ABE   |ATL        |01051245|01/05  12:45|\n",
      "|01050605|9    |602     |ABE   |ATL        |01050605|01/05  06:05|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT *, date, to_date_format_udf(date) AS date_fm FROM us_delay_flights_tbl\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f7e1748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TEMPORARY VIEW delay_temp2 AS (SELECT *, date, to_date_format_udf(date) AS date_fm FROM us_delay_flights_tbl WHERE date IS NOT NULL)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf004917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|date    |delay|distance|origin|destination|date    |date_fm     |\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|01011245|6    |602     |ABE   |ATL        |01011245|01/01  12:45|\n",
      "|01020600|-8   |369     |ABE   |DTW        |01020600|01/02  06:00|\n",
      "|01021245|-2   |602     |ABE   |ATL        |01021245|01/02  12:45|\n",
      "|01020605|-4   |602     |ABE   |ATL        |01020605|01/02  06:05|\n",
      "|01031245|-4   |602     |ABE   |ATL        |01031245|01/03  12:45|\n",
      "|01030605|0    |602     |ABE   |ATL        |01030605|01/03  06:05|\n",
      "|01041243|10   |602     |ABE   |ATL        |01041243|01/04  12:43|\n",
      "|01040605|28   |602     |ABE   |ATL        |01040605|01/04  06:05|\n",
      "|01051245|88   |602     |ABE   |ATL        |01051245|01/05  12:45|\n",
      "|01050605|9    |602     |ABE   |ATL        |01050605|01/05  06:05|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM delay_temp2\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9b4dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+--------+-----+\n",
      "|delay|origin|destination|    date|Month|\n",
      "+-----+------+-----------+--------+-----+\n",
      "|    6|   ABE|        ATL|01011245| null|\n",
      "|   -8|   ABE|        DTW|01020600| null|\n",
      "|   -2|   ABE|        ATL|01021245| null|\n",
      "|   -4|   ABE|        ATL|01020605| null|\n",
      "|   -4|   ABE|        ATL|01031245| null|\n",
      "|    0|   ABE|        ATL|01030605| null|\n",
      "|   10|   ABE|        ATL|01041243| null|\n",
      "|   28|   ABE|        ATL|01040605| null|\n",
      "|   88|   ABE|        ATL|01051245| null|\n",
      "|    9|   ABE|        ATL|01050605| null|\n",
      "+-----+------+-----------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT delay, origin, destination, date, MONTH(date_fm) AS Month FROM delay_temp2\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37616014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we want to label all US flights with an indication of the delays: Very Long Delays, Long Delays...\n",
    "\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c95e2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All SQL queries can be expressed with an equivalent Data‐Frame API query\n",
    "\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(col(\"distance\") > 1000)\n",
    " .orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdafe000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "(df.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    " .where(col(\"delay\") > 120)\n",
    " .where(col(\"origin\") == \"SFO\")\n",
    " .where(col(\"destination\") == \"ORD\")\n",
    " .orderBy(desc(\"delay\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e258c9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+----------------+\n",
      "|    date|delay|distance|origin|destination|   Flight_Delays|\n",
      "+--------+-----+--------+------+-----------+----------------+\n",
      "|01011245|    6|     602|   ABE|        ATL|Tolerable delays|\n",
      "|01020600|   -8|     369|   ABE|        DTW|           Early|\n",
      "|01021245|   -2|     602|   ABE|        ATL|           Early|\n",
      "|01020605|   -4|     602|   ABE|        ATL|           Early|\n",
      "|01031245|   -4|     602|   ABE|        ATL|           Early|\n",
      "|01030605|    0|     602|   ABE|        ATL|       No delays|\n",
      "|01041243|   10|     602|   ABE|        ATL|Tolerable delays|\n",
      "|01040605|   28|     602|   ABE|        ATL|Tolerable delays|\n",
      "|01051245|   88|     602|   ABE|        ATL|    Short delays|\n",
      "|01050605|    9|     602|   ABE|        ATL|Tolerable delays|\n",
      "+--------+-----+--------+------+-----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.withColumn(\"Flight_Delays\",\n",
    "                    when(col(\"delay\") > 360, \"Very long delays\")\n",
    "                    .when((df.delay > 120) & (df.delay < 360), \"Long delays\")\n",
    "                    .when((df.delay > 60) & (df.delay < 120), \"Short delays\")\n",
    "                    .when((df.delay > 0) & (df.delay < 60), \"Tolerable delays\")\n",
    "                    .when((df.delay > 120) & (df.delay < 360), \"Long delays\")\n",
    "                    .when(df.delay == 0, \"No delays\")\n",
    "                    .otherwise(\"Early\")\n",
    "                   ).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71b835",
   "metadata": {},
   "source": [
    "##### SQL Tables and Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6c750d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "939b1c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a managed table\n",
    "\n",
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150bcf17",
   "metadata": {},
   "source": [
    "You can do the same thing using the DataFrame API like this:\n",
    "\n",
    "#In Python\n",
    "#Path to US flight delays CSV file \n",
    "\n",
    "csv_file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\")\n",
    "     \n",
    "#Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0c521",
   "metadata": {},
   "source": [
    "Create a unmanaged table\n",
    "You can create unmanaged tables from your own data sources—say, Par‐quet, CSV, or JSON files stored in a file store accessible to your Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69464bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an unmanaged table\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n",
    " distance INT, origin STRING, destination STRING) \n",
    " USING csv OPTIONS (PATH \n",
    " 'C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62586bc0",
   "metadata": {},
   "source": [
    "#Within the DataFrame API use:\n",
    "\n",
    "(flights_df\n",
    " .write\n",
    " .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    " .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01f80fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a global temporary view with SQL\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'SFO';\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5cb1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "776c5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can accomplish the same thing with the DataFrame API as follows:\n",
    "\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "396a810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When accessing a global temporary view you must use the prefix global_temp.<view_name>\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62347918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can access the normal temporary view without the global_temp prefix\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM us_origin_airport_JFK_tmp_view\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb2c3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can simply use SQL to query the table and assign the returned result to a DataFrame:\n",
    "\n",
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78149971",
   "metadata": {},
   "source": [
    "Spark manages the metadata associated with each managed or unmanaged table. This is captured in the Catalog, a high-level abstraction in Spark SQL for storing metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b4507c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/C:/Windows/system32/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Windows/system32/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can access all the stored metadata\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae37a3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='firecalls', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='delay_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5105ce4",
   "metadata": {},
   "source": [
    "DataFrameWriter \n",
    "Saves or writes data to a specified built-in data source. Unlike with DataFrameReader, you access its instance not from a \n",
    "SparkSession but from the DataFrame you wish to save."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3f499",
   "metadata": {},
   "source": [
    "PARQUET is an open source columnar file format, it's the default data source in Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c364ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a DataFrame to Parquet file\n",
    "\n",
    "(df.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9474bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Spark SQL table\n",
    "\n",
    "(df.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77427bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a CSV file into a DataFrame\n",
    "\n",
    "file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "df = (spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .schema(schema)\n",
    " .option(\"mode\", \"FAILFAST\") # Exit if any errors\n",
    " .option(\"nullValue\", \"\") # Replace any null data field with quotes\n",
    " .load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7304af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SQL table from a CSV data source (it is no different from Parquet or JSON)\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    " USING csv\n",
    " OPTIONS (\n",
    " path \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n",
    " header \"true\",\n",
    " inferSchema \"true\",\n",
    " mode \"FAILFAST\"\n",
    " )\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adbd41ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00318800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a DataFrame as a CSV file\n",
    "\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dd1bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read an ORC file into a DataFrame\n",
    "\n",
    "file = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a569458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read an ORC file into a Spark SQL table\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    " USING orc\n",
    " OPTIONS (\n",
    " path \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\",\n",
    " header \"true\",\n",
    " inferSchema \"true\",\n",
    " mode \"FAILFAST\"\n",
    " )\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eff76a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a DataFrame to ORC file\n",
    "\n",
    "(df.write.format(\"orc\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/orc/flights_orc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19ffe2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading an image file into a DataFrame\n",
    "\n",
    "from pyspark.ml import image\n",
    "\n",
    "image_dir = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ac8683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+-----+\n",
      "|height|width|nChannels|mode|label|\n",
      "+------+-----+---------+----+-----+\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |1    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "+------+-----+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    " \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc455410",
   "metadata": {},
   "source": [
    "The DataFrameReader converts each binary file into a single DataFrame row (record) that contains the raw content and metadata of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "589887cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|                path|    modificationTime|length|             content|label|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 55037|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54634|[FF D8 FF E0 00 1...|    1|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54624|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54505|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54475|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a binary file into a DataFrame\n",
    "\n",
    "path = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "300a5d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|                path|    modificationTime|length|             content|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 55037|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54634|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54624|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54505|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2022-04-26 18:34:...| 54475|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .option(\"recursiveFileLookup\", \"true\")\n",
    " .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c7a71",
   "metadata": {},
   "source": [
    "The binary file data source does not support writing a DataFrame back to the original file format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74707a37",
   "metadata": {},
   "source": [
    "Leer los AVRO, Parquet, JSON y CSV escritos en el cap.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a520a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+----------------+----------+----------+-----+----------------------+---------------------------+---+-----+----+----+----+----+----+----+-----+----+----+------+----+----+----+---------------------+-------------------------------------+-------------+---------+\n",
      "|_c0     |_c1|_c2    |_c3             |_c4       |_c5       |_c6  |_c7                   |_c8                        |_c9|_c10 |_c11|_c12|_c13|_c14|_c15|_c16|_c17 |_c18|_c19|_c20  |_c21|_c22|_c23|_c24                 |_c25                                 |_c26         |_c27     |\n",
      "+--------+---+-------+----------------+----------+----------+-----+----------------------+---------------------------+---+-----+----+----+----+----+----+----+-----+----+----+------+----+----+----+---------------------+-------------------------------------+-------------+---------+\n",
      "|20110016|T13|2003235|Structure Fire  |01/11/2002|01/10/2002|Other|01/11/2002 01:51:44 AM|2000 Block of CALIFORNIA ST|SF |94109|B04 |38  |3362|3   |3   |3   |false|null|1   |TRUCK |2   |4   |5   |Pacific Heights      |(37.7895840679362, -122.428071912459)|020110016-T13|2.95     |\n",
      "|20110022|M17|2003241|Medical Incident|01/11/2002|01/10/2002|Other|01/11/2002 03:01:18 AM|0 Block of SILVERVIEW DR   |SF |94124|B10 |42  |6495|3   |3   |3   |true |null|1   |MEDIC |1   |10  |10  |Bayview Hunters Point|(37.7337623673897, -122.396113802632)|020110022-M17|4.7      |\n",
      "|20110023|M41|2003242|Medical Incident|01/11/2002|01/10/2002|Other|01/11/2002 02:39:50 AM|MARKET ST/MCALLISTER ST    |SF |94102|B03 |01  |1455|3   |3   |3   |true |null|1   |MEDIC |2   |3   |6   |Tenderloin           |(37.7811772186856, -122.411699931232)|020110023-M41|2.4333334|\n",
      "|20110032|E11|2003250|Vehicle Fire    |01/11/2002|01/10/2002|Other|01/11/2002 04:16:46 AM|APPLETON AV/MISSION ST     |SF |94110|B06 |32  |5626|3   |3   |3   |false|null|1   |ENGINE|1   |6   |9   |Bernal Heights       |(37.7388432849018, -122.423948785199)|020110032-E11|1.5      |\n",
      "|20110043|B04|2003259|Alarms          |01/11/2002|01/10/2002|Other|01/11/2002 06:01:58 AM|1400 Block of SUTTER ST    |SF |94109|B04 |03  |3223|3   |3   |3   |false|null|1   |CHIEF |2   |4   |2   |Western Addition     |(37.7872890372638, -122.424236212664)|020110043-B04|3.4833333|\n",
      "+--------+---+-------+----------------+----------+----------+-----+----------------------+---------------------------+---+-----+----+----+----+----+----+----+-----+----+----+------+----+----+----+---------------------+-------------------------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"C:/tmp/output/fire_csv/test-fire.csv/*\"\n",
    "df = spark.read.format(\"csv\").option(\"path\", file).load()\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70bca7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+----------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+-------------------------------------+---------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "|ALSUnit|Address                    |AvailableDtTm         |Battalion|Box |CallDate  |CallFinalDisposition|CallNumber|CallType        |CallTypeGroup|City|Delay    |FinalPriority|FirePreventionDistrict|IncidentNumber|Location                             |Neighborhood         |NumAlarms|OriginalPriority|Priority|RowID        |StationArea|SupervisorDistrict|UnitID|UnitSequenceInCallDispatch|UnitType|WatchDate |Zipcode|\n",
      "+-------+---------------------------+----------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+-------------------------------------+---------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "|false  |2000 Block of CALIFORNIA ST|01/11/2002 01:51:44 AM|B04      |3362|01/11/2002|Other               |20110016  |Structure Fire  |null         |SF  |2.95     |3            |4                     |2003235       |(37.7895840679362, -122.428071912459)|Pacific Heights      |1        |3               |3       |020110016-T13|38         |5                 |T13   |2                         |TRUCK   |01/10/2002|94109  |\n",
      "|true   |0 Block of SILVERVIEW DR   |01/11/2002 03:01:18 AM|B10      |6495|01/11/2002|Other               |20110022  |Medical Incident|null         |SF  |4.7      |3            |10                    |2003241       |(37.7337623673897, -122.396113802632)|Bayview Hunters Point|1        |3               |3       |020110022-M17|42         |10                |M17   |1                         |MEDIC   |01/10/2002|94124  |\n",
      "|true   |MARKET ST/MCALLISTER ST    |01/11/2002 02:39:50 AM|B03      |1455|01/11/2002|Other               |20110023  |Medical Incident|null         |SF  |2.4333334|3            |3                     |2003242       |(37.7811772186856, -122.411699931232)|Tenderloin           |1        |3               |3       |020110023-M41|01         |6                 |M41   |2                         |MEDIC   |01/10/2002|94102  |\n",
      "|false  |APPLETON AV/MISSION ST     |01/11/2002 04:16:46 AM|B06      |5626|01/11/2002|Other               |20110032  |Vehicle Fire    |null         |SF  |1.5      |3            |6                     |2003250       |(37.7388432849018, -122.423948785199)|Bernal Heights       |1        |3               |3       |020110032-E11|32         |9                 |E11   |1                         |ENGINE  |01/10/2002|94110  |\n",
      "|false  |1400 Block of SUTTER ST    |01/11/2002 06:01:58 AM|B04      |3223|01/11/2002|Other               |20110043  |Alarms          |null         |SF  |3.4833333|3            |4                     |2003259       |(37.7872890372638, -122.424236212664)|Western Addition     |1        |3               |3       |020110043-B04|03         |2                 |B04   |2                         |CHIEF   |01/10/2002|94109  |\n",
      "+-------+---------------------------+----------------------+---------+----+----------+--------------------+----------+----------------+-------------+----+---------+-------------+----------------------+--------------+-------------------------------------+---------------------+---------+----------------+--------+-------------+-----------+------------------+------+--------------------------+--------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"C:/tmp/output/fire_json/*\"\n",
    "df = spark.read.format(\"json\").option(\"path\", file).load()\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "23e0cc0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+----------------------+------------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+------------------+-------------------------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|CallType        |CallDate  |WatchDate |CallFinalDisposition|AvailableDtTm         |Address                 |City|Zipcode|Battalion|StationArea|Box |OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|Neighborhood      |Location                             |RowID        |Delay    |\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+----------------------+------------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+------------------+-------------------------------------+-------------+---------+\n",
      "|110660238 |60    |11021821      |Medical Incident|03/07/2011|03/07/2011|Fire                |03/07/2011 04:23:05 PM|0 Block of BORICA ST    |SF  |94127  |B09      |19         |8442|3               |2       |2            |true   |null         |1        |MEDIC   |1                         |9                     |7                 |West of Twin Peaks|(37.7225922041408, -122.468227979374)|110660238-60 |2.9      |\n",
      "|110660253 |B03   |11021831      |Alarms          |03/07/2011|03/07/2011|Fire                |03/07/2011 05:13:21 PM|1000 Block of HOWARD ST |SF  |94103  |B03      |01         |2252|3               |3       |3            |false  |null         |1        |CHIEF   |3                         |3                     |6                 |South of Market   |(37.7795675906982, -122.407474134289)|110660253-B03|2.3833334|\n",
      "|110660282 |E43   |11021856      |Medical Incident|03/07/2011|03/07/2011|Other               |03/07/2011 06:55:52 PM|PERSIA AV/MADRID ST     |SF  |94112  |B09      |43         |6134|3               |3       |3            |true   |null         |1        |ENGINE  |1                         |9                     |11                |Excelsior         |(37.7216187707613, -122.432805977203)|110660282-E43|1.65     |\n",
      "|110660322 |94    |11021888      |Medical Incident|03/07/2011|03/07/2011|Other               |03/07/2011 08:49:25 PM|1100 Block of HOWARD ST |SF  |94103  |B03      |01         |2314|3               |3       |3            |true   |null         |1        |MEDIC   |3                         |2                     |6                 |South of Market   |(37.7768521020734, -122.410711239368)|110660322-94 |1.5666667|\n",
      "|110670051 |B09   |11021963      |Alarms          |03/08/2011|03/07/2011|Fire                |03/08/2011 07:36:00 AM|5800 Block of MISSION ST|SF  |94112  |B09      |33         |6213|3               |3       |3            |false  |null         |1        |CHIEF   |3                         |9                     |11                |Outer Mission     |(37.709160030699, -122.451801431228) |110670051-B09|2.8      |\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+----------------------+------------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+------------------+-------------------------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"C:/tmp/output/fire_parquet/*\"\n",
    "df = spark.read.format(\"parquet\").option(\"path\", file).load()\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d1c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

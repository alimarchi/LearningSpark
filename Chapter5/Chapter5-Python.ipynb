{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a31bf55",
   "metadata": {},
   "source": [
    "#### UDF User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e14cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a54d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cubed function\n",
    "\n",
    "def cubed(s):\n",
    " return s * s * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b03b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(s)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register UDF\n",
    "\n",
    "spark.udf.register(\"cubed\", cubed, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0720a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate temporary view\n",
    "\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bb189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Spark SQL to execute the cubed() function\n",
    "\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8640b",
   "metadata": {},
   "source": [
    "One of the previous prevailing issues with using PySpark UDFs was that they had slower performance than Scala UDFs. \n",
    "To resolve this problem, Pandas UDFs (also known as vectorized UDFs) were introduced as part of Apache Spark 2.3.\n",
    "A Pandas UDF uses Apache Arrow to transfer data and Pandas to work with the data. \n",
    "You define a Pandas UDF using the keyword pandas_udf as the decorator, or to wrap the function itself.\n",
    "Instead of operating on individual inputs row by row, you are operating on a Pandas Series or DataFrame.\n",
    "\n",
    "From Apache Spark 3.0 with Python 3.6 and above, Pandas UDFs were split into two API categories: \n",
    "* Pandas UDFs \n",
    "* Pandas Function APIs: allow you to directly apply a local Python function to a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126ab487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "tripdelaysFilePath = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "airportsnaFilePath = \"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2//flights/airport-codes-na.txt\"\n",
    " \n",
    "# Obtain airports data set\n",
    "\n",
    "airportsna = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    " .load(airportsnaFilePath))\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "# Obtain departure delays data set\n",
    "\n",
    "departureDelays = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\")\n",
    " .load(tripdelaysFilePath))\n",
    "departureDelays = (departureDelays\n",
    " .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "# Create temporary small table\n",
    "\n",
    "foo = (departureDelays\n",
    " .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    " date like '01010%' and delay > 0\"\"\")))\n",
    "foo.createOrReplaceTempView(\"foo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1f24d",
   "metadata": {},
   "source": [
    "The departureDelays DataFrame contains data on >1.3M flights while the foo DataFrame contains just three rows with information \n",
    "on flights from SEA to SFO for a specific time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747b2202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324f2a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe297493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e45a5f",
   "metadata": {},
   "source": [
    "#### Union\n",
    "A common pattern within Apache Spark is to union two different DataFrames with the same schema together. This can be achieved using the union() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b63be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union two tables\n",
    "\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "\n",
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d705d6",
   "metadata": {},
   "source": [
    "The bar DataFrame is the union of foo with delays. Using the same filtering criteria results in the bar DataFrame, we see a duplication of the foo data, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4585066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    " FROM bar \n",
    " WHERE origin = 'SEA' \n",
    " AND destination = 'SFO' \n",
    " AND date LIKE '01010%' \n",
    " AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08842279",
   "metadata": {},
   "source": [
    "#### Joins\n",
    "A common DataFrame operation is to join two DataFrames (or tables) together. By default, a Spark SQL join is an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f60843af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join departure delays data (foo) with airport info\n",
    "\n",
    "foo.join(\n",
    " airportsna, \n",
    " airportsna.IATA == foo.origin\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37567593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \n",
    " FROM foo f\n",
    " JOIN airports_na a\n",
    " ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d84581",
   "metadata": {},
   "source": [
    "#### Spark MySQL\n",
    "Loading data from a JDBC source using load.\n",
    "(Cargar con spark datos de empleados y departamentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d805090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "employees = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"employees\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"root1234-\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0dd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83369b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac98892",
   "metadata": {},
   "outputs": [],
   "source": [
    "departments = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"departments\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"root1234-\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f59790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_no|         dept_name|\n",
      "+-------+------------------+\n",
      "|   d009|  Customer Service|\n",
      "|   d005|       Development|\n",
      "|   d002|           Finance|\n",
      "|   d003|   Human Resources|\n",
      "|   d001|         Marketing|\n",
      "|   d004|        Production|\n",
      "|   d006|Quality Management|\n",
      "|   d008|          Research|\n",
      "|   d007|             Sales|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca2b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"salaries\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"root1234-\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84b46d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4828b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"titles\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"root1234-\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf4505a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+----------+\n",
      "|emp_no|          title| from_date|   to_date|\n",
      "+------+---------------+----------+----------+\n",
      "| 10001|Senior Engineer|1986-06-26|9999-01-01|\n",
      "| 10002|          Staff|1996-08-03|9999-01-01|\n",
      "| 10003|Senior Engineer|1995-12-03|9999-01-01|\n",
      "| 10004|       Engineer|1986-12-01|1995-12-01|\n",
      "| 10004|Senior Engineer|1995-12-01|9999-01-01|\n",
      "| 10005|   Senior Staff|1996-09-12|9999-01-01|\n",
      "| 10005|          Staff|1989-09-12|1996-09-12|\n",
      "| 10006|Senior Engineer|1990-08-05|9999-01-01|\n",
      "| 10007|   Senior Staff|1996-02-11|9999-01-01|\n",
      "| 10007|          Staff|1989-02-10|1996-02-11|\n",
      "+------+---------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2563bdd",
   "metadata": {},
   "source": [
    "Mediante Joins mostrar toda la información de los empleados además de su título y salario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d68ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+----------------+\n",
      "|first_name|last_name|birth_date|           title|\n",
      "+----------+---------+----------+----------------+\n",
      "|  Alassane|  Iwayama|1960-09-19|Technique Leader|\n",
      "|   Shalesh|  dAstous|1963-09-16|    Senior Staff|\n",
      "|Aleksander|   Danlos|1953-07-11|        Engineer|\n",
      "|Aleksander|   Danlos|1953-07-11| Senior Engineer|\n",
      "|       Uri|  Rullman|1958-10-02|    Senior Staff|\n",
      "|       Uri|  Rullman|1958-10-02|           Staff|\n",
      "|   Shushma|     Bahk|1957-03-01|        Engineer|\n",
      "|   Shushma|     Bahk|1957-03-01| Senior Engineer|\n",
      "|   Vasiliy|Kermarrec|1957-08-20|        Engineer|\n",
      "|   Vasiliy|Kermarrec|1957-08-20| Senior Engineer|\n",
      "+----------+---------+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(titles,\n",
    " employees.emp_no == titles.emp_no).select(\"first_name\", \"last_name\", \"birth_date\", \"title\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "979d40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------+\n",
      "|first_name|last_name|birth_date|salary|\n",
      "+----------+---------+----------+------+\n",
      "|  Alassane|  Iwayama|1960-09-19| 40000|\n",
      "|  Alassane|  Iwayama|1960-09-19| 43519|\n",
      "|  Alassane|  Iwayama|1960-09-19| 46265|\n",
      "|  Alassane|  Iwayama|1960-09-19| 46865|\n",
      "|  Alassane|  Iwayama|1960-09-19| 47837|\n",
      "|  Alassane|  Iwayama|1960-09-19| 52042|\n",
      "|  Alassane|  Iwayama|1960-09-19| 52370|\n",
      "|  Alassane|  Iwayama|1960-09-19| 53202|\n",
      "|  Alassane|  Iwayama|1960-09-19| 56087|\n",
      "|  Alassane|  Iwayama|1960-09-19| 59252|\n",
      "+----------+---------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(salaries,\n",
    " employees.emp_no == salaries.emp_no).select(\"first_name\", \"last_name\", \"birth_date\", \"salary\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3858bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------+----------------+\n",
      "|first_name|last_name|birth_date|salary|           title|\n",
      "+----------+---------+----------+------+----------------+\n",
      "|  Alassane|  Iwayama|1960-09-19| 40000|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 43519|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 46265|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 46865|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 47837|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 52042|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 52370|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 53202|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 56087|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 59252|Technique Leader|\n",
      "+----------+---------+----------+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(salaries,\n",
    " employees.emp_no == salaries.emp_no).join(titles, employees.emp_no == titles.emp_no).select(\"first_name\", \"last_name\", \"birth_date\", \"salary\", \"title\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "944ae2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.createOrReplaceTempView(\"employees_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "170572e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM employees_tb\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2941c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries.createOrReplaceTempView(\"salaries_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0a70d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.createOrReplaceTempView(\"titles_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e20cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------+----------------+\n",
      "|first_name|last_name|birth_date|salary|           title|\n",
      "+----------+---------+----------+------+----------------+\n",
      "|  Alassane|  Iwayama|1960-09-19| 40000|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 43519|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 46265|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 46865|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 47837|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 52042|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 52370|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 53202|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 56087|Technique Leader|\n",
      "|  Alassane|  Iwayama|1960-09-19| 59252|Technique Leader|\n",
      "+----------+---------+----------+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.first_name, a.last_name, a.birth_date, b.salary, c.title \n",
    " FROM employees_tb AS a\n",
    " JOIN salaries_tb AS b\n",
    " ON a.emp_no = b.emp_no\n",
    " JOIN titles_tb AS c\n",
    " ON a.emp_no = c.emp_no\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbe90b",
   "metadata": {},
   "source": [
    "##### Diferencia entre Rank y dense_rank (operaciones de ventana) \n",
    "\n",
    "##### RANK vs DENSE_RANK \n",
    "\n",
    "Se usan para ordenar valores y asignarles unos números. \n",
    "(Ejemplo, si tenemos 3 estudiantes con 3 notas diferentes, 100, 85 y 72, esta función asignará los números 1,2 y 3 a cada estudiante según cómo hemos decidido rankearlo). \n",
    "\n",
    "Ambas funciones usan una sentencia OVER() con PARTITION BY y ORDER BY. \n",
    "(PARTITION BY es opcional pero ORDER BY es obligatorio).\n",
    "\n",
    "SELECT student_name, RANK() OVER(ORDER BY grades DESC) AS grade_ranking\n",
    "\n",
    "PARTITION BY agrupa los rankings. Cuando los valores cambian en la columna especificada, el ranking vuelve a empezar. \n",
    "\n",
    "Ejemplo: SELECT student_name, DENSE_RANK() OVER(PARTITION BY subject ORDER BY grades DESC) AS grade_ranking\n",
    "```\n",
    "\n",
    "Diferencia --> Si por ejemplo 2 estudiantes tienen la misma nota \n",
    "                  RANK    DENSE RANK       \n",
    "Jessica 76          4\t\t 3\n",
    "Madison 100\t        1\t\t 1\n",
    "Sebastian 100\t    1\t\t 1\n",
    "Eric 92 \t        3        2\n",
    "Josephine 63 \t    5        4\n",
    "\n",
    "```\n",
    "\n",
    "RANK salta el número 2, mientras DENSE_RANK no se salta ningún valor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2735faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------+----------+----------+----------+\n",
      "|first_name|last_name|birth_date|salary| from_date|   to_date|last_value|\n",
      "+----------+---------+----------+------+----------+----------+----------+\n",
      "|     Aamer| Feinberg|1953-11-25| 45996|1992-11-02|1993-11-02|     45996|\n",
      "|     Aamer| Feinberg|1953-11-25| 46979|1993-11-02|1994-11-02|     46979|\n",
      "|     Aamer| Feinberg|1953-11-25| 49729|1994-11-02|1995-11-02|     49729|\n",
      "|     Aamer| Feinberg|1953-11-25| 50212|1995-11-02|1996-11-01|     50212|\n",
      "|     Aamer| Feinberg|1953-11-25| 51062|1996-11-01|1997-11-01|     51062|\n",
      "|     Aamer| Feinberg|1953-11-25| 51370|1997-11-01|1998-11-01|     51370|\n",
      "|     Aamer| Feinberg|1953-11-25| 55258|1998-11-01|1999-11-01|     55258|\n",
      "|     Aamer| Feinberg|1953-11-25| 59075|1999-11-01|2000-10-31|     59075|\n",
      "|     Aamer| Feinberg|1953-11-25| 60199|2000-10-31|2001-10-31|     60199|\n",
      "|     Aamer| Feinberg|1953-11-25| 59974|2001-10-31|9999-01-01|     59974|\n",
      "|     Aamer| Molenaar|1957-10-27| 79194|1986-04-22|1987-04-22|     79194|\n",
      "|     Aamer| Molenaar|1957-10-27| 83486|1987-04-22|1988-04-21|     83486|\n",
      "|     Aamer| Molenaar|1957-10-27| 84941|1988-04-21|1989-04-21|     84941|\n",
      "|     Aamer| Molenaar|1957-10-27| 87522|1989-04-21|1990-04-21|     87522|\n",
      "|     Aamer| Molenaar|1957-10-27| 87402|1990-04-21|1991-04-21|     87402|\n",
      "|     Aamer| Molenaar|1957-10-27| 90510|1991-04-21|1992-04-20|     90510|\n",
      "|     Aamer| Molenaar|1957-10-27| 94589|1992-04-20|1993-04-20|     94589|\n",
      "|     Aamer| Molenaar|1957-10-27| 94899|1993-04-20|1994-04-20|     94899|\n",
      "|     Aamer| Molenaar|1957-10-27| 95661|1994-04-20|1995-04-20|     95661|\n",
      "|     Aamer| Molenaar|1957-10-27| 98546|1995-04-20|1996-04-19|     98546|\n",
      "+----------+---------+----------+------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.first_name, a.last_name, a.birth_date, b.salary, b.from_date, b.to_date,\n",
    "LAST_VALUE(b.salary) OVER (PARTITION BY a.first_name, a.last_name ORDER BY from_date) AS last_value\n",
    " FROM employees_tb AS a\n",
    " JOIN salaries_tb AS b\n",
    " ON a.emp_no = b.emp_no\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2644dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+---------------+----------+----------+----+\n",
      "|first_name|   last_name|salary|          title| from_date|   to_date|rank|\n",
      "+----------+------------+------+---------------+----------+----------+----+\n",
      "|     Aamer|    Feinberg| 59974|   Senior Staff|2001-10-31|9999-01-01|   1|\n",
      "|     Aamer|    Feinberg| 59974|          Staff|2001-10-31|9999-01-01|   1|\n",
      "|     Aamer|    Feinberg| 60199|   Senior Staff|2000-10-31|2001-10-31|   2|\n",
      "|     Aamer|    Feinberg| 60199|          Staff|2000-10-31|2001-10-31|   2|\n",
      "|     Aamer|    Feinberg| 59075|   Senior Staff|1999-11-01|2000-10-31|   3|\n",
      "|     Aamer|    Feinberg| 59075|          Staff|1999-11-01|2000-10-31|   3|\n",
      "|     Aamer|    Molenaar|115331|   Senior Staff|2002-04-18|9999-01-01|   1|\n",
      "|     Aamer|    Molenaar|115331|          Staff|2002-04-18|9999-01-01|   1|\n",
      "|     Aamer|    Molenaar|114295|   Senior Staff|2001-04-18|2002-04-18|   2|\n",
      "|     Aamer|    Molenaar|114295|          Staff|2001-04-18|2002-04-18|   2|\n",
      "|     Aamer|    Molenaar|110631|   Senior Staff|2000-04-18|2001-04-18|   3|\n",
      "|     Aamer|    Molenaar|110631|          Staff|2000-04-18|2001-04-18|   3|\n",
      "| Abdelaziz|       Rosin| 53248|       Engineer|2001-02-17|2001-06-13|   1|\n",
      "| Abdelaziz|       Rosin| 50838|       Engineer|2000-02-18|2001-02-17|   2|\n",
      "| Abdelaziz|       Rosin| 49572|       Engineer|1999-02-18|2000-02-18|   3|\n",
      "|Abdelghani|Bernardeschi| 54691|       Engineer|2002-07-08|9999-01-01|   1|\n",
      "|Abdelghani|Bernardeschi| 60020|       Engineer|2001-08-21|9999-01-01|   1|\n",
      "|Abdelghani|Bernardeschi| 60020|Senior Engineer|2001-08-21|9999-01-01|   1|\n",
      "|Abdelghani|Bernardeschi| 52146|       Engineer|2001-07-08|2002-07-08|   2|\n",
      "|Abdelghani|Bernardeschi| 58696|       Engineer|2000-08-21|2001-08-21|   3|\n",
      "+----------+------------+------+---------------+----------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    " SELECT first_name, last_name, salary, title, from_date, to_date, rank\n",
    " FROM ( \n",
    " SELECT a.first_name, a.last_name, b.salary, c.title, b.from_date, b.to_date, \n",
    " dense_rank() OVER (PARTITION BY a.first_name, a.last_name ORDER BY b.to_date DESC) AS rank\n",
    " FROM employees_tb AS a\n",
    " JOIN salaries_tb AS b\n",
    " ON a.emp_no = b.emp_no\n",
    " JOIN titles_tb AS c\n",
    " ON a.emp_no = c.emp_no)\n",
    " WHERE rank <= 3\n",
    " \"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52780b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_emp = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"dept_emp\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"root1234-\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c03a99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "| 10003|   d004|1995-12-03|9999-01-01|\n",
      "| 10004|   d004|1986-12-01|9999-01-01|\n",
      "| 10005|   d003|1989-09-12|9999-01-01|\n",
      "| 10006|   d005|1990-08-05|9999-01-01|\n",
      "| 10007|   d008|1989-02-10|9999-01-01|\n",
      "| 10008|   d005|1998-03-11|2000-07-31|\n",
      "| 10009|   d006|1985-02-18|9999-01-01|\n",
      "| 10010|   d004|1996-11-24|2000-06-26|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_emp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b543125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_emp.createOrReplaceTempView(\"dept_emp_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a03b7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.createOrReplaceTempView(\"departments_tb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28369e72",
   "metadata": {},
   "source": [
    "Utilizando operaciones de ventana obtener el salario, posición (cargo) y departamento actual de cada empleado, es decir, el último o más reciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97e8e3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+---------------+----------------+----------+----------+----+\n",
      "|first_name|   last_name|salary|          title|       dept_name| from_date|   to_date|rank|\n",
      "+----------+------------+------+---------------+----------------+----------+----------+----+\n",
      "|     Aamer|    Feinberg| 59974|   Senior Staff|         Finance|2001-10-31|9999-01-01|   1|\n",
      "|     Aamer|    Feinberg| 59974|          Staff|         Finance|2001-10-31|9999-01-01|   1|\n",
      "|     Aamer|    Molenaar|115331|   Senior Staff|           Sales|2002-04-18|9999-01-01|   1|\n",
      "|     Aamer|    Molenaar|115331|          Staff|           Sales|2002-04-18|9999-01-01|   1|\n",
      "| Abdelaziz|       Rosin| 53248|       Engineer|     Development|2001-02-17|2001-06-13|   1|\n",
      "|Abdelghani|Bernardeschi| 54691|       Engineer|     Development|2002-07-08|9999-01-01|   1|\n",
      "|Abdelghani|Bernardeschi| 60020|       Engineer|      Production|2001-08-21|9999-01-01|   1|\n",
      "|Abdelghani|Bernardeschi| 60020|Senior Engineer|      Production|2001-08-21|9999-01-01|   1|\n",
      "|Abdelghani|Bernardeschi| 54691|       Engineer|        Research|2002-07-08|9999-01-01|   1|\n",
      "|Abdelghani|      Leuchs| 72884|       Engineer|     Development|2001-08-10|9999-01-01|   1|\n",
      "|Abdelghani|      Leuchs| 72884|Senior Engineer|     Development|2001-08-10|9999-01-01|   1|\n",
      "|Abdelghani|        Luca| 69313|   Senior Staff|        Research|2002-04-18|9999-01-01|   1|\n",
      "|Abdelghani|        Luca| 69313|          Staff|        Research|2002-04-18|9999-01-01|   1|\n",
      "|Abdelwaheb|        Pero| 75753|       Engineer|     Development|2002-04-19|9999-01-01|   1|\n",
      "|Abdelwaheb|     Zumaque| 70857|Senior Engineer|     Development|2001-08-27|9999-01-01|   1|\n",
      "|Abdelwaheb|     Zumaque| 70857|Senior Engineer|        Research|2001-08-27|9999-01-01|   1|\n",
      "|   Abdulah|      Kopetz| 74928|       Engineer|     Development|2002-05-30|9999-01-01|   1|\n",
      "|   Abdulah|      Kopetz| 74928|Senior Engineer|     Development|2002-05-30|9999-01-01|   1|\n",
      "|   Abdulah|     Kushner| 47944|          Staff| Human Resources|2002-06-28|9999-01-01|   1|\n",
      "|   Abdulla| Radivojevic| 65535|   Senior Staff|Customer Service|2001-11-15|9999-01-01|   1|\n",
      "+----------+------------+------+---------------+----------------+----------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    " SELECT first_name, last_name, salary, title, dept_name, from_date, to_date, rank\n",
    " FROM ( \n",
    " SELECT a.first_name, a.last_name, b.salary, c.title, e.dept_name, b.from_date, b.to_date,\n",
    " dense_rank() OVER (PARTITION BY a.first_name, a.last_name ORDER BY b.to_date DESC) AS rank\n",
    " FROM employees_tb AS a\n",
    " JOIN salaries_tb AS b\n",
    " ON a.emp_no = b.emp_no\n",
    " JOIN titles_tb AS c\n",
    " ON a.emp_no = c.emp_no\n",
    " JOIN dept_emp_tb AS d\n",
    " ON a.emp_no = d.emp_no\n",
    " JOIN departments_tb AS e \n",
    " ON d.dept_no = e.dept_no)\n",
    " WHERE rank = 1\n",
    " \"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb678fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505e27f6",
   "metadata": {},
   "source": [
    "Authors ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cc0105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://L2203100.bosonit.local:4043\n",
       "SparkContext available as 'sc' (version = 3.0.3, master = local[*], app id = local-1651070667389)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.avg\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@55d059b1\r\n",
       "dataDF: org.apache.spark.sql.DataFrame = [name: string, age: int]\r\n",
       "avgDF: org.apache.spark.sql.DataFrame = [name: string, avg(age): double]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame using a SparkSession\n",
    "\n",
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"AuthorsAges\")\n",
    "    .getOrCreate()\n",
    "\n",
    "val dataDF = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Brooke\", 25),\n",
    " (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")\n",
    "\n",
    "val avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))\n",
    "\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e845011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(author,StringType,false), StructField(title,StringType,false), StructField(pages,IntegerType,false))\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Defino el schema\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "val schema = StructType(Array(StructField(\"author\", StringType, false),\n",
    " StructField(\"title\", StringType, false),\n",
    " StructField(\"pages\", IntegerType, false)))\n",
    "\n",
    "                         \n",
    "// Usando DDL (Data Definition Language)\n",
    "// val schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6c978",
   "metadata": {},
   "source": [
    "Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116505a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@55d059b1\r\n",
       "jsonFile: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/chapter3/scala/data/blogs.json\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read data from a JSON file\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"Example-3_7\")\n",
    " .getOrCreate()\n",
    "    \n",
    "val jsonFile = (\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/chapter3/scala/data/blogs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea09e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|Id |First    |Last   |Url              |Published|Hits |Campaigns                   |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|1  |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter, LinkedIn]         |\n",
      "|2  |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter, LinkedIn]         |\n",
      "|3  |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter, FB, LinkedIn]|\n",
      "|4  |Tathagata|Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]               |\n",
      "|5  |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB, LinkedIn]|\n",
      "|6  |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter, LinkedIn]         |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "()\n",
      "StructType(StructField(Id,IntegerType,true), StructField(First,StringType,true), StructField(Last,StringType,true), StructField(Url,StringType,true), StructField(Published,StringType,true), StructField(Hits,IntegerType,true), StructField(Campaigns,ArrayType(StringType,true),true))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,IntegerType,false), StructField(First,StringType,false), StructField(Last,StringType,false), StructField(Url,StringType,false), StructField(Published,StringType,false), StructField(Hits,IntegerType,false), StructField(Campaigns,ArrayType(StringType,true),false))\r\n",
       "blogsDF: org.apache.spark.sql.DataFrame = [Id: int, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Defino el schema\n",
    "\n",
    "val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    " StructField(\"First\", StringType, false),\n",
    " StructField(\"Last\", StringType, false),\n",
    " StructField(\"Url\", StringType, false),\n",
    " StructField(\"Published\", StringType, false),\n",
    " StructField(\"Hits\", IntegerType, false),\n",
    " StructField(\"Campaigns\", ArrayType(StringType), false)))\n",
    "\n",
    "// Creo un DataFrame leyendo el file json y usando un schema predefinido\n",
    "\n",
    "val blogsDF = spark.read.schema(schema).json(jsonFile)\n",
    "\n",
    "blogsDF.show(false)\n",
    "println(blogsDF.printSchema)\n",
    "println(blogsDF.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76ceec",
   "metadata": {},
   "source": [
    "Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9200cad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csv: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/chapter2/scala/data/mnm_dataset.csv\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Subo el fichero MnMs\n",
    "\n",
    "val csv = (\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/chapter2/scala/data/mnm_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198dc7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(StructField(State,StringType,true), StructField(Color,StringType,true), StructField(Count,IntegerType,true))\n",
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvDF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Obtengo la estructura del schema dado por defecto\n",
    "\n",
    "val csvDF = spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(csv)\n",
    "\n",
    "println(csvDF.schema)\n",
    "println(csvDF.printSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714c438",
   "metadata": {},
   "source": [
    "El último parámetro booleano es \"nullable\", si el campo puede ser nulo o no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36d51662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "res6: Array[String] = Array(Id, First, Last, Url, Published, Hits, Campaigns)\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Ejemplo blog\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "blogsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66008d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.Column = Id\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Accedo a una columna con col\n",
    "blogsDF.col(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6396744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Uso col para calcular un valor\n",
    "\n",
    "blogsDF.select(col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e1d19db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Añado una nueva columna llamada Big Hitters, se basa en una expresión condicional\n",
    "\n",
    "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a7fc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Puedo concatenar 3 columnas para crear una columna\n",
    "\n",
    "blogsDF\n",
    " .withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\"))))\n",
    " .select(col(\"AuthorsId\"))\n",
    " .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8632a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Los 3 me devuelven el mismo resultado, expr es lo mismo que col \n",
    "\n",
    "blogsDF.select(expr(\"Hits\")).show(2)\n",
    "blogsDF.select(col(\"Hits\")).show(2)\n",
    "blogsDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52748031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// col(\"Id\") devuleve la columna y es lo mismo que usar $ antes del nombre de la columna, que es una \n",
    "// función de Spark que devuelve Id en una columna\n",
    "\n",
    "blogsDF.sort(col(\"Id\").desc).show()\n",
    "blogsDF.sort($\"Id\".desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f2fbc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "blogRow: org.apache.spark.sql.Row = [6,Reynold,Xin,https://tinyurl.6,255568,3/2/2015,[Ljava.lang.String;@458fd97a]\r\n",
       "res14: Any = Reynold\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A row in Spark is a generic Row object, containing one or more columns. \n",
    "\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val blogRow = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", Array(\"twitter\", \"LinkedIn\"))\n",
    "\n",
    "blogRow(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1287913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rows: Seq[(String, String)] = List((Matei Zaharia,CA), (Reynold Xin,CA))\r\n",
       "authorsDF: org.apache.spark.sql.DataFrame = [Author: string, State: string]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creo un DataFrame usando un objeto Row\n",
    "\n",
    "val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\"))\n",
    "val authorsDF = rows.toDF(\"Author\", \"State\")\n",
    "authorsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27b737",
   "metadata": {},
   "source": [
    "Fire calls example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f82234a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType, BooleanType}\r\n",
       "fireSchema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,IntegerType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Para importar un fichero csv con DataFrameReader, primero defino el schema\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType, BooleanType};\n",
    "\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "     StructField(\"UnitID\", StringType, true),\n",
    "     StructField(\"IncidentNumber\", IntegerType, true),\n",
    "     StructField(\"CallType\", StringType, true),\n",
    "     StructField(\"CallDate\", StringType, true), \n",
    "     StructField(\"WatchDate\", StringType, true),\n",
    "     StructField(\"CallFinalDisposition\", StringType, true),\n",
    "     StructField(\"AvailableDtTm\", StringType, true),\n",
    "     StructField(\"Address\", StringType, true), \n",
    "     StructField(\"City\", StringType, true), \n",
    "     StructField(\"Zipcode\", IntegerType, true), \n",
    "     StructField(\"Battalion\", StringType, true), \n",
    "     StructField(\"StationArea\", StringType, true), \n",
    "     StructField(\"Box\", StringType, true), \n",
    "     StructField(\"OriginalPriority\", StringType, true), \n",
    "     StructField(\"Priority\", StringType, true), \n",
    "     StructField(\"FinalPriority\", IntegerType, true), \n",
    "     StructField(\"ALSUnit\", BooleanType, true), \n",
    "     StructField(\"CallTypeGroup\", StringType, true),\n",
    "     StructField(\"NumAlarms\", IntegerType, true),\n",
    "     StructField(\"UnitType\", StringType, true),\n",
    "     StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "     StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "     StructField(\"SupervisorDistrict\", StringType, true),\n",
    "     StructField(\"Neighborhood\", StringType, true),                             \n",
    "     StructField(\"Location\", StringType, true),\n",
    "     StructField(\"RowID\", StringType, true),\n",
    "     StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bec6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sfFireFile: String = C:/Users/alice.marchi/Downloads/LearningSparkV2-master/chapter3/data/sf-fire-calls.csv\r\n",
       "fireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sfFireFile=\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/chapter3/data/sf-fire-calls.csv\"\n",
    "val fireDF = spark.read.schema(fireSchema)\n",
    " .option(\"header\", \"true\")\n",
    " .csv(sfFireFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db871920",
   "metadata": {},
   "source": [
    "Guardar los datos en: parquet, json, csv, avro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683861b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parquetPath: String = /tmp/output/fire_parquet1\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquetPath = \"/tmp/output/fire_parquet\"\n",
    "fireDF.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9838a7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parquet_table: String = tbl_fire\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquet_table = \"tbl_fire\"\n",
    "fireDF.write.format(\"parquet\").saveAsTable(parquet_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0ecb879",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.write.format(\"json\").save(\"/tmp/output/fire_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f5f3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.write.format(\"csv\").save(\"/tmp/output/fire_csv/test-fire.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29b3430e",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:676)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)\r",
      "  at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:966)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:303)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\r",
      "  ... 36 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "fireDF.write.format(\"avro\").save(\"/tmp/output/fire_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c589a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Int = 8\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ¿Cómo obtener el número de particiones de un DataFrame?\n",
    "fireDF.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad04a6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_fireDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// modificar el número de particiones de un DataFrame\n",
    "val new_fireDF = fireDF.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0c7d722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Int = 1\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_fireDF.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aba8042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "//  Llevar a cabo el ejemplo modificando el número de particiones a 1 y revisar de nuevo el/los ficheros guardados. \n",
    "\n",
    "new_fireDF.write.format(\"json\").save(\"/tmp/output/new_fire_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5daf054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fewFireDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [IncidentNumber: int, AvailableDtTm: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fewFireDF = fireDF\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where($\"CallType\" =!= \"Medical Incident\") \n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555a74d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull)\n",
    " .agg(countDistinct('CallType) as 'DistinctCallTypes)\n",
    " .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f151970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where($\"ResponseDelayedinMins\" > 5)\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5661df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fireTsDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireTsDF = newFireDF\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\")\n",
    "\n",
    "fireTsDF\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816a0ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    " .select(year($\"IncidentDate\"))\n",
    " .distinct()\n",
    " .orderBy(year($\"IncidentDate\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c623e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull)\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(desc(\"count\"))\n",
    " .show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb98a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{functions=>F}\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{functions => F}\n",
    "fireTsDF\n",
    " .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),\n",
    " F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b32fde",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9ebc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "row: org.apache.spark.sql.Row = [350,true,Learning Spark 2E,null]\r\n",
       "res0: String = Learning Spark 2E\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Row is a generic object type in Spark, holding a collection of mixed types that can be accessed using an index. \n",
    "\n",
    "import org.apache.spark.sql.Row\n",
    "val row = Row(350, true, \"Learning Spark 2E\", null)\n",
    "\n",
    "row.getInt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d281f506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Boolean = true\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.getBoolean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823357aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: String = Learning Spark 2E\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.getString(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18bea2d",
   "metadata": {},
   "source": [
    "CREATING DATASET \n",
    "\n",
    "As with creating DataFrames from data sources, when creating a Dataset you have to know the schema. In other words, you need to know the data types. Although with JSON and CSV data it’s possible to infer the schema, for large data sets this is resource-intensive (expensive). When creating a Dataset in Scala, the easiest way to specify the schema for the resulting Dataset is to use a case class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "298ddf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceIoTData\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a Scala case class \n",
    "\n",
    "case class DeviceIoTData (battery_level: Long, c02_level: Long,\n",
    "cca2: String, cca3: String, cn: String, device_id: Long,\n",
    "device_name: String, humidity: Long, ip: String, latitude: Double,\n",
    "lcd: String, longitude: Double, scale:String, temp: Long,\n",
    "timestamp: Long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b24a4c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceTempByCountry\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long, cca3: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d0aec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|cn           |device_id|device_name          |humidity|ip           |latitude|lcd   |longitude|scale  |temp|timestamp    |\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|8            |868      |US  |USA |United States|1        |meter-gauge-1xbYRYcj |51      |68.161.225.1 |38.0    |green |-97.0    |Celsius|34  |1458444054093|\n",
      "|7            |1473     |NO  |NOR |Norway       |2        |sensor-pad-2n2Pea    |70      |213.161.254.1|62.47   |red   |6.15     |Celsius|11  |1458444054119|\n",
      "|2            |1556     |IT  |ITA |Italy        |3        |device-mac-36TWSKiT  |44      |88.36.5.1    |42.83   |red   |12.83    |Celsius|19  |1458444054120|\n",
      "|6            |1080     |US  |USA |United States|4        |sensor-pad-4mzWkz    |32      |66.39.173.154|44.06   |yellow|-121.32  |Celsius|28  |1458444054121|\n",
      "|4            |931      |PH  |PHL |Philippines  |5        |therm-stick-5gimpUrBB|62      |203.82.41.9  |14.58   |green |120.97   |Celsius|25  |1458444054122|\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Once defined we can use it to read our file and convert the returned Dataset[Row] into Dataset[DeviceIoTData]\n",
    "\n",
    "val ds = spark.read\n",
    ".json(\"C:/Users/alice.marchi/Downloads/LearningSparkV2-master/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json\")\n",
    ".as[DeviceIoTData]\n",
    "\n",
    "ds.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ed551b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- battery_level: long (nullable = true)\n",
      " |-- c02_level: long (nullable = true)\n",
      " |-- cca2: string (nullable = true)\n",
      " |-- cca3: string (nullable = true)\n",
      " |-- cn: string (nullable = true)\n",
      " |-- device_id: long (nullable = true)\n",
      " |-- device_name: string (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lcd: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- scale: string (nullable = true)\n",
      " |-- temp: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44c0a486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterTempDS: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterTempDS = ds.filter(d => {d.temp > 30 && d.humidity > 70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56945b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
